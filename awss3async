import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import software.amazon.awssdk.auth.credentials.AwsCredentials;
import software.amazon.awssdk.auth.credentials.AwsCredentialsProvider;
import software.amazon.awssdk.auth.credentials.AwsSessionCredentials;
import software.amazon.awssdk.services.s3.S3Client;

@Configuration
public class S3ClientConfig {

    @Bean
    public S3Client s3Client() {
        AwsCredentialsProvider credentialsProvider = new AwsCredentialsProvider() {
            @Override
            public AwsCredentials resolveCredentials() {
                // Fetch a new token dynamically
                String token = fetchNewToken();
                return AwsSessionCredentials.create("accessKey", "secretKey", token);
            }
        };

        return S3Client.builder()
                .credentialsProvider(credentialsProvider)
                .build();
    }

    private String fetchNewToken() {
        // Implement the logic to fetch a new token from your token provider or authentication service
        // Example: Call your identity provider or any external API
        return "newTokenHere";
    }
}

public boolean multipartDownloadAndUpload(String sourceBucket, String sourceKey, String destBucket, String destKey) {
    // Step 1: Initiate the multipart upload for the destination bucket
    CreateMultipartUploadResponse createMultipartUploadResponse = s3Client.createMultipartUpload(b -> b
        .bucket(destBucket)
        .key(destKey));
    String uploadId = createMultipartUploadResponse.uploadId();

    List<CompletedPart> completedParts = new ArrayList<>();
    int partNumber = 1;

    try {
        // Step 2: Get the size of the source file
        HeadObjectResponse headObjectResponse = s3Client.headObject(b -> b
            .bucket(sourceBucket)
            .key(sourceKey));
        long fileSize = headObjectResponse.contentLength();

        // Step 3: Download and upload in chunks
        long position = 0;
        while (position < fileSize) {
            long endByte = Math.min(position + CHUNK_SIZE - 1, fileSize - 1); // Calculate byte range

            // Download chunk from the source bucket
            String range = "bytes=" + position + "-" + endByte;
            logger.info("Downloading range: {}", range);
            GetObjectRequest getObjectRequest = GetObjectRequest.builder()
                .bucket(sourceBucket)
                .key(sourceKey)
                .range(range)
                .build();

            ByteBuffer chunkBuffer = s3Client.getObject(getObjectRequest, ResponseTransformer.toBytes()).asByteBuffer();

            // Upload the chunk to the destination bucket
            UploadPartRequest uploadPartRequest = UploadPartRequest.builder()
                .bucket(destBucket)
                .key(destKey)
                .uploadId(uploadId)
                .partNumber(partNumber)
                .contentLength((long) chunkBuffer.remaining())
                .build();

            UploadPartResponse uploadPartResponse = s3Client.uploadPart(uploadPartRequest, RequestBody.fromByteBuffer(chunkBuffer));

            // Add the completed part
            CompletedPart part = CompletedPart.builder()
                .partNumber(partNumber)
                .eTag(uploadPartResponse.eTag())
                .build();
            completedParts.add(part);

            // Move to the next chunk
            position += CHUNK_SIZE;
            partNumber++;
        }

        // Step 4: Complete the multipart upload
        s3Client.completeMultipartUpload(b -> b
            .bucket(destBucket)
            .key(destKey)
            .uploadId(uploadId)
            .multipartUpload(CompletedMultipartUpload.builder().parts(completedParts).build()));

        logger.info("File successfully transferred from {}:{} to {}:{}", sourceBucket, sourceKey, destBucket, destKey);
        return true;
    } catch (Exception e) {
        // Abort the multipart upload if any error occurs
        s3Client.abortMultipartUpload(b -> b
            .bucket(destBucket)
            .key(destKey)
            .uploadId(uploadId));
        logger.error("Failed to transfer file from {}:{} to {}:{}. Error: {}", sourceBucket, sourceKey, destBucket, destKey, e.getMessage(), e);
        return false;
    }
}


---/-------------------------------111-111111111111111111111111111111111111111111111


import software.amazon.awssdk.services.s3.*;
import software.amazon.awssdk.services.s3.model.*;
import software.amazon.awssdk.core.async.*;
import java.nio.ByteBuffer;
import java.util.concurrent.*;
import java.util.*;
import java.util.stream.*;

public class S3ChunkedFileTransfer {

    private static final String SOURCE_BUCKET = "source-bucket";
    private static final String DEST_BUCKET = "destination-bucket";
    private static final String SOURCE_FILE_KEY = "large-file.mp4"; // Same key for source and destination
    private static final int CHUNK_SIZE = 8 * 1024 * 1024; // 8 MB
    private static final int MAX_CONCURRENT_OPERATIONS = 5;

    private static final S3AsyncClient s3AsyncClient = S3AsyncClient.create();
    private static final ExecutorService executorService = Executors.newFixedThreadPool(MAX_CONCURRENT_OPERATIONS);

    public static void main(String[] args) throws Exception {
        transferFileInChunks();
        executorService.shutdown();
    }

    public static void transferFileInChunks() throws Exception {
        // Step 1: Get the file size from the source bucket
        long fileSize = getFileSize(SOURCE_BUCKET, SOURCE_FILE_KEY);

        // Step 2: Split the file into chunks based on chunk size
        List<Long> chunkOffsets = LongStream.iterate(0, offset -> offset + CHUNK_SIZE)
                .limit((fileSize + CHUNK_SIZE - 1) / CHUNK_SIZE)
                .boxed()
                .collect(Collectors.toList());

        // Step 3: Initialize the multipart upload
        CreateMultipartUploadRequest createMultipartRequest = CreateMultipartUploadRequest.builder()
                .bucket(DEST_BUCKET)
                .key(SOURCE_FILE_KEY) // Keep the same file name
                .build();
        
        CreateMultipartUploadResponse createMultipartResponse = s3AsyncClient.createMultipartUpload(createMultipartRequest).get();
        String uploadId = createMultipartResponse.uploadId();

        // Step 4: Process each chunk (download and upload) in parallel
        List<CompletableFuture<Void>> tasks = chunkOffsets.stream()
                .map(offset -> CompletableFuture.runAsync(() -> {
                    try {
                        processChunk(SOURCE_BUCKET, SOURCE_FILE_KEY, DEST_BUCKET, uploadId, offset, fileSize);
                    } catch (Exception e) {
                        System.err.println("Error processing chunk at offset " + offset + ": " + e.getMessage());
                    }
                }, executorService))
                .collect(Collectors.toList());

        // Wait for all tasks to complete
        CompletableFuture.allOf(tasks.toArray(new CompletableFuture[0])).join();

        // Step 5: Complete the multipart upload
        completeMultipartUpload(uploadId);

        System.out.println("File transfer completed.");
    }

    // Generalized method to get file size
    private static long getFileSize(String bucket, String key) throws Exception {
        HeadObjectRequest headRequest = HeadObjectRequest.builder().bucket(bucket).key(key).build();
        return s3AsyncClient.headObject(headRequest).get().contentLength();
    }

    // Common method to process the chunk (download and upload)
    private static void processChunk(String sourceBucket, String sourceKey, String destBucket, String uploadId, long startByte, long fileSize) throws Exception {
        // Adjust endByte to ensure it does not exceed file size
        long endByte = Math.min(startByte + CHUNK_SIZE - 1, fileSize - 1);

        System.out.println("Processing chunk: " + startByte + " - " + endByte);

        // Step 1: Download the chunk from the source bucket
        ByteBuffer chunkData = downloadChunk(sourceBucket, sourceKey, startByte, endByte);

        // Ensure chunk size is as expected
        long chunkSize = chunkData.remaining();
        if (chunkSize != (endByte - startByte + 1)) {
            System.err.println("Warning: Chunk size mismatch, expected " + (endByte - startByte + 1) + " bytes, but got " + chunkSize + " bytes.");
        }

        // Step 2: Upload the chunk to the destination bucket using the same file name
        uploadChunk(destBucket, sourceKey, uploadId, chunkData, startByte);

        System.out.println("Successfully processed chunk: " + startByte + " - " + endByte);
    }

    // Common method to download a chunk
    private static ByteBuffer downloadChunk(String bucket, String key, long startByte, long endByte) throws Exception {
        // Ensure that the byte range is correctly calculated
        String byteRange = "bytes=" + startByte + "-" + endByte;
        System.out.println("Downloading range: " + byteRange);

        GetObjectRequest getRequest = GetObjectRequest.builder()
                .bucket(bucket)
                .key(key)
                .range(byteRange)
                .build();

        // Download the chunk and return the ByteBuffer
        return s3AsyncClient.getObject(getRequest, AsyncResponseTransformer.toBytes()).get().asByteBuffer();
    }

    // Common method to upload a chunk as part of a multipart upload
    private static void uploadChunk(String bucket, String key, String uploadId, ByteBuffer chunkData, long startByte) throws Exception {
        // Ensure that content length matches the chunk size
        long chunkSize = chunkData.remaining();

        // Step 1: Create part upload request
        UploadPartRequest uploadPartRequest = UploadPartRequest.builder()
                .bucket(bucket)
                .key(key)
                .uploadId(uploadId)
                .partNumber((int) (startByte / CHUNK_SIZE) + 1) // Part number starts from 1
                .contentLength(chunkSize)
                .build();

        // Step 2: Upload the part
        UploadPartResponse uploadPartResponse = s3AsyncClient.uploadPart(uploadPartRequest, AsyncRequestBody.fromByteBuffer(chunkData)).get();

        // Step 3: Track the uploaded part
        System.out.println("Uploaded chunk part: " + uploadPartResponse.eTag());
    }

    // Complete the multipart upload
    private static void completeMultipartUpload(String uploadId) throws Exception {
        // Step 1: List uploaded parts
        List<CompletedPart> completedParts = new ArrayList<>();
        // You would collect the parts here (in the case of parallel execution, track the eTag and part number)

        // Step 2: Complete multipart upload
        CompleteMultipartUploadRequest completeMultipartRequest = CompleteMultipartUploadRequest.builder()
                .bucket(DEST_BUCKET)
                .key(SOURCE_FILE_KEY)
                .uploadId(uploadId)
                .multipartUpload(CompletedMultipartUpload.builder().parts(completedParts).build())
                .build();

        s3AsyncClient.completeMultipartUpload(completeMultipartRequest).get();
        System.out.println("Multipart upload completed successfully.");
    }
}

-------------------------------------------------------

import software.amazon.awssdk.core.async.AsyncRequestBody;
import software.amazon.awssdk.core.async.AsyncResponseTransformer;
import software.amazon.awssdk.services.s3.S3AsyncClient;
import software.amazon.awssdk.services.s3.model.*;
import java.nio.ByteBuffer;
import java.nio.file.*;
import java.util.concurrent.CompletableFuture;

public class S3ChunkTransfer {
    private static final int CHUNK_SIZE = 5 * 1024 * 1024; // 5 MB
    private static final int MAX_RETRIES = 3;

    public static void main(String[] args) throws Exception {
        String sourceBucket = "source-bucket-name";
        String sourceKey = "source/audio/file.mp3";
        String destinationBucket = "destination-bucket-name";
        String destinationKey = "destination/audio/file.mp3";

        S3AsyncClient s3Client = S3AsyncClient.create();
        transferFileInChunks(s3Client, sourceBucket, sourceKey, destinationBucket, destinationKey);
    }

    private static void transferFileInChunks(S3AsyncClient s3Client, String sourceBucket, String sourceKey,
                                             String destinationBucket, String destinationKey) throws Exception {
        // Get the total size of the file
        HeadObjectResponse headObject = s3Client.headObject(
            HeadObjectRequest.builder().bucket(sourceBucket).key(sourceKey).build()
        ).join();
        long totalSize = headObject.contentLength();

        // Transfer chunks
        for (long offset = 0; offset < totalSize; offset += CHUNK_SIZE) {
            boolean success = false;
            int retries = 0;

            while (!success && retries < MAX_RETRIES) {
                try {
                    // Download the chunk
                    ByteBuffer chunkData = downloadChunk(s3Client, sourceBucket, sourceKey, offset, CHUNK_SIZE, totalSize);

                    // Upload the chunk
                    uploadChunk(s3Client, destinationBucket, destinationKey, offset, chunkData);

                    success = true;
                } catch (Exception e) {
                    retries++;
                    if (retries >= MAX_RETRIES) {
                        System.err.printf("Failed to transfer chunk at offset %d after %d retries.%n", offset, retries);
                        throw e;
                    }
                    System.err.printf("Retrying chunk at offset %d (%d/%d).%n", offset, retries, MAX_RETRIES);
                }
            }
        }
        System.out.println("File transferred successfully!");
    }

    private static ByteBuffer downloadChunk(S3AsyncClient s3Client, String bucket, String key, long offset, int chunkSize, long totalSize) throws Exception {
        long end = Math.min(offset + chunkSize - 1, totalSize - 1);

        CompletableFuture<ByteBuffer> future = s3Client.getObject(
            GetObjectRequest.builder()
                .bucket(bucket)
                .key(key)
                .range(String.format("bytes=%d-%d", offset, end))
                .build(),
            AsyncResponseTransformer.toBytes()
        ).thenApply(response -> response.asByteBuffer());

        return future.join();
    }

    private static void uploadChunk(S3AsyncClient s3Client, String bucket, String key, long offset, ByteBuffer chunkData) throws Exception {
        CompletableFuture<Void> future = s3Client.putObject(
            PutObjectRequest.builder()
                .bucket(bucket)
                .key(key)
                .build(),
            AsyncRequestBody.fromBytes(chunkData.array())
        );

        future.join();
    }
}


-----------------------------------


import software.amazon.awssdk.services.s3.model.*;
import software.amazon.awssdk.services.s3.*;
import software.amazon.awssdk.core.async.*;
import java.nio.file.*;
import java.nio.ByteBuffer;
import java.util.concurrent.*;
import java.util.List;

public class S3ChunkUpload {

    private static final String SOURCE_BUCKET = "source-bucket";
    private static final String DEST_BUCKET = "destination-bucket";
    private static final String FILE_KEY = "audio-file.mp3";
    private static final int CHUNK_SIZE = 5 * 1024 * 1024; // 5MB chunks (can adjust)

    private static S3AsyncClient s3AsyncClient = S3AsyncClient.create();
    private static ExecutorService executorService = Executors.newFixedThreadPool(4);

    public static void main(String[] args) {
        try {
            // Step 1: Download the file in chunks
            downloadAndUploadInChunks(FILE_KEY);
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            executorService.shutdown();
        }
    }

    public static void downloadAndUploadInChunks(String key) throws InterruptedException, ExecutionException {
        // Step 2: Get file metadata from the source bucket
        HeadObjectRequest headRequest = HeadObjectRequest.builder()
                .bucket(SOURCE_BUCKET)
                .key(key)
                .build();

        HeadObjectResponse headResponse = s3AsyncClient.headObject(headRequest).get();
        long fileSize = headResponse.contentLength();

        // Step 3: Download file in chunks
        for (long startByte = 0; startByte < fileSize; startByte += CHUNK_SIZE) {
            long endByte = Math.min(startByte + CHUNK_SIZE - 1, fileSize - 1);
            System.out.println("Downloading chunk: " + startByte + " to " + endByte);

            // Download chunk asynchronously
            downloadChunk(key, startByte, endByte).thenAcceptAsync(chunkData -> {
                // Step 4: Upload chunk to destination bucket
                uploadChunk(chunkData, startByte).thenAccept(response -> {
                    System.out.println("Chunk uploaded successfully: " + startByte);
                }).exceptionally(e -> {
                    System.err.println("Error uploading chunk: " + startByte + " due to: " + e.getMessage());
                    return null;
                });
            }).exceptionally(e -> {
                System.err.println("Error downloading chunk: " + startByte + " due to: " + e.getMessage());
                return null;
            });
        }
    }

    public static CompletableFuture<ByteBuffer> downloadChunk(String key, long startByte, long endByte) {
        GetObjectRequest getRequest = GetObjectRequest.builder()
                .bucket(SOURCE_BUCKET)
                .key(key)
                .range("bytes=" + startByte + "-" + endByte)
                .build();

        CompletableFuture<ByteBuffer> future = new CompletableFuture<>();
        s3AsyncClient.getObject(getRequest, AsyncResponseTransformer.toBytes())
                .whenComplete((response, exception) -> {
                    if (exception != null) {
                        future.completeExceptionally(exception);
                    } else {
                        future.complete(response.asByteBuffer());
                    }
                });
        return future;
    }

    public static CompletableFuture<PutObjectResponse> uploadChunk(ByteBuffer chunkData, long startByte) {
        String destKey = FILE_KEY + ".part" + startByte;

        PutObjectRequest putRequest = PutObjectRequest.builder()
                .bucket(DEST_BUCKET)
                .key(destKey)
                .build();

        return s3AsyncClient.putObject(putRequest, AsyncRequestBody.fromByteBuffer(chunkData));
    }
}
----
public static CompletableFuture<PutObjectResponse> uploadChunkWithRetry(ByteBuffer chunkData, long startByte, int retries) {
    return uploadChunk(chunkData, startByte).handle((response, exception) -> {
        if (exception != null && retries > 0) {
            System.out.println("Retrying chunk upload: " + startByte);
            return uploadChunkWithRetry(chunkData, startByte, retries - 1);
        }
        return response;
    }).thenCompose(response -> response);
}
------------------
import software.amazon.awssdk.services.s3.*;
import software.amazon.awssdk.services.s3.model.*;
import software.amazon.awssdk.core.async.*;
import java.nio.ByteBuffer;
import java.util.concurrent.*;
import java.util.stream.*;
import java.util.*;

public class S3ChunkedTransfer {

    private static final String SOURCE_BUCKET = "source-bucket";
    private static final String DEST_BUCKET = "destination-bucket";
    private static final String SOURCE_FILE_KEY = "audio-file.mp3";
    private static final String DEST_FILE_KEY = "audio-file.mp3";
    private static final int CHUNK_SIZE = 8 * 1024 * 1024; // 8 MB
    private static final int MAX_CONCURRENT_OPERATIONS = 5;

    private static final S3AsyncClient s3AsyncClient = S3AsyncClient.create();
    private static final ExecutorService executorService = Executors.newFixedThreadPool(MAX_CONCURRENT_OPERATIONS);

    public static void main(String[] args) throws Exception {
        transferFileInChunks();
        executorService.shutdown();
    }

    public static void transferFileInChunks() throws Exception {
        // Step 1: Get file metadata from the source bucket
        long fileSize = getFileSize(SOURCE_BUCKET, SOURCE_FILE_KEY);

        // Step 2: Determine the chunks
        List<Long> chunkOffsets = LongStream.iterate(0, offset -> offset + CHUNK_SIZE)
                .limit((fileSize + CHUNK_SIZE - 1) / CHUNK_SIZE)
                .boxed()
                .collect(Collectors.toList());

        // Step 3: Process each chunk (download and upload) in parallel
        List<CompletableFuture<Void>> tasks = chunkOffsets.stream()
                .map(offset -> CompletableFuture.runAsync(() -> {
                    try {
                        processChunk(offset, Math.min(offset + CHUNK_SIZE - 1, fileSize - 1));
                    } catch (Exception e) {
                        System.err.println("Error processing chunk at offset " + offset + ": " + e.getMessage());
                        throw new CompletionException(e);
                    }
                }, executorService))
                .collect(Collectors.toList());

        // Wait for all tasks to complete
        CompletableFuture.allOf(tasks.toArray(new CompletableFuture[0])).join();
        System.out.println("File transfer completed.");
    }

import software.amazon.awssdk.services.s3.*;
import software.amazon.awssdk.services.s3.model.*;
import software.amazon.awssdk.core.async.*;
import java.nio.ByteBuffer;
import java.util.concurrent.*;
import java.util.stream.*;
import java.util.*;

public class S3ChunkedTransfer {

    private static final String SOURCE_BUCKET = "source-bucket";
    private static final String DEST_BUCKET = "destination-bucket";
    private static final String SOURCE_FILE_KEY = "audio-file.mp3";
    private static final String DEST_FILE_KEY = "audio-file.mp3";
    private static final int CHUNK_SIZE = 8 * 1024 * 1024; // 8 MB
    private static final int MAX_CONCURRENT_OPERATIONS = 5;

    private static final S3AsyncClient s3AsyncClient = S3AsyncClient.create();
    private static final ExecutorService executorService = Executors.newFixedThreadPool(MAX_CONCURRENT_OPERATIONS);

    public static void main(String[] args) throws Exception {
        transferFileInChunks();
        executorService.shutdown();
    }

    public static void transferFileInChunks() throws Exception {
        // Step 1: Get file metadata from the source bucket
        long fileSize = getFileSize(SOURCE_BUCKET, SOURCE_FILE_KEY);

        // Step 2: Determine the chunks
        List<Long> chunkOffsets = LongStream.iterate(0, offset -> offset + CHUNK_SIZE)
                .limit((fileSize + CHUNK_SIZE - 1) / CHUNK_SIZE)
                .boxed()
                .collect(Collectors.toList());

        // Step 3: Process each chunk (download and upload) in parallel
        List<CompletableFuture<Void>> tasks = chunkOffsets.stream()
                .map(offset -> CompletableFuture.runAsync(() -> {
                    try {
                        processChunk(offset, Math.min(offset + CHUNK_SIZE - 1, fileSize - 1));
                    } catch (Exception e) {
                        System.err.println("Error processing chunk at offset " + offset + ": " + e.getMessage());
                        throw new CompletionException(e);
                    }
                }, executorService))
                .collect(Collectors.toList());

        // Wait for all tasks to complete
        CompletableFuture.allOf(tasks.toArray(new CompletableFuture[0])).join();
        System.out.println("File transfer completed.");
    }

    private static long getFileSize(String bucket, String key) throws Exception {
        HeadObjectRequest headRequest = HeadObjectRequest.builder().bucket(bucket).key(key).build();
        return s3AsyncClient.headObject(headRequest).get().contentLength();
    }

    private static void processChunk(long startByte, long endByte) throws Exception {
        System.out.println("Processing chunk: " + startByte + " - " + endByte);

        // Step 1: Download the chunk from the source bucket
        ByteBuffer chunkData = downloadChunk(SOURCE_BUCKET, SOURCE_FILE_KEY, startByte, endByte);

        // Step 2: Upload the chunk to the destination bucket
        uploadChunk(DEST_BUCKET, DEST_FILE_KEY, chunkData, startByte);

        System.out.println("Successfully processed chunk: " + startByte + " - " + endByte);
    }

    private static ByteBuffer downloadChunk(String bucket, String key, long startByte, long endByte) throws Exception {
        GetObjectRequest getRequest = GetObjectRequest.builder()
                .bucket(bucket)
                .key(key)
                .range("bytes=" + startByte + "-" + endByte)
                .build();

        return s3AsyncClient.getObject(getRequest, AsyncResponseTransformer.toBytes()).get().asByteBuffer();
    }

    private static void uploadChunk(String bucket, String key, ByteBuffer chunkData, long startByte) throws Exception {
        String partKey = key + ".part" + startByte; // Use a unique part key if needed

        PutObjectRequest putRequest = PutObjectRequest.builder()
                .bucket(bucket)
                .key(partKey)
                .build();

        s3AsyncClient.putObject(putRequest, AsyncRequestBody.fromByteBuffer(chunkData)).get();
        System.out.println("Uploaded chunk starting at byte: " + startByte);
    }
}

