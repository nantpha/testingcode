
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.model.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import javax.annotation.PreDestroy;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicLong;

@Service
public class OptimizedS3Processor {
    private static final Logger log = LoggerFactory.getLogger(OptimizedS3Processor.class);
    
    private final AmazonS3 s3Client;
    private final ExecutorService processingExecutor;
    private final AtomicLong totalProcessed = new AtomicLong(0);
    
    @Value("${chunk.size:300}")
    private int chunkSize;
    
    @Value("${max.files:1000000}")
    private long maxFiles;
    
    @Value("${max.threads:32}")
    private int maxThreads;
    
    @Value("${processing.timeout.minutes:60}")
    private long timeoutMinutes;

    @Autowired
    public OptimizedS3Processor(AmazonS3 s3Client) {
        this.s3Client = s3Client;
        this.processingExecutor = new ThreadPoolExecutor(
            Runtime.getRuntime().availableProcessors(),
            maxThreads,
            60L, TimeUnit.SECONDS,
            new LinkedBlockingQueue<>(1000),
            new ThreadPoolExecutor.CallerRunsPolicy()
        );
    }

    @Async
    public CompletableFuture<Void> execute(String bucketName, String prefix) {
        final long startTime = System.currentTimeMillis();
        
        return CompletableFuture.runAsync(() -> {
            String continuationToken = null;
            do {
                if (isProcessingComplete() || isTimeout(startTime)) break;
                
                ListObjectsV2Result result = s3Client.listObjectsV2(new ListObjectsV2Request()
                    .withBucketName(bucketName)
                    .withPrefix(prefix)
                    .withMaxKeys(1000)
                    .withContinuationToken(continuationToken));
                
                processResultBatch(result.getObjectSummaries());
                continuationToken = result.getNextContinuationToken();
                
            } while (continuationToken != null && !isProcessingComplete() && !isTimeout(startTime));
            
        }, processingExecutor).whenComplete((v, t) -> {
            if (t != null) {
                log.error("Processing failed for {}/{}", bucketName, prefix, t);
            } else {
                log.info("Processed {} files from {}/{} in {} minutes",
                    totalProcessed.get(),
                    bucketName,
                    prefix,
                    (System.currentTimeMillis() - startTime) / 60000);
            }
        });
    }

    private void processResultBatch(List<S3ObjectSummary> objects) {
        List<CompletableFuture<Void>> futures = new ArrayList<>();
        
        for (int i = 0; i < objects.size(); i += chunkSize) {
            if (isProcessingComplete()) break;
            
            List<S3ObjectSummary> chunk = objects.subList(i, Math.min(i + chunkSize, objects.size()));
            futures.add(processChunkAsync(chunk));
        }
        
        // Wait for current batch chunks to complete
        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();
    }

    private CompletableFuture<Void> processChunkAsync(List<S3ObjectSummary> chunk) {
        return CompletableFuture.runAsync(() -> {
            try {
                // Replace with actual processing logic
                processChunk(chunk);
                totalProcessed.addAndGet(chunk.size());
                
                if (totalProcessed.get() % 10000 == 0) {
                    log.info("Progress: {} files processed", totalProcessed.get());
                }
            } catch (Exception e) {
                log.error("Chunk processing failed", e);
                // Implement retry logic or dead-letter queue here
            }
        }, processingExecutor);
    }

    private void processChunk(List<S3ObjectSummary> chunk) {
        // Your actual chunk processing logic
        // Consider implementing batch database operations or parallel processing within chunks
    }

    private boolean isProcessingComplete() {
        return totalProcessed.get() >= maxFiles;
    }

    private boolean isTimeout(long startTime) {
        return (System.currentTimeMillis() - startTime) > TimeUnit.MINUTES.toMillis(timeoutMinutes);
    }

    @PreDestroy
    public void shutdown() {
        try {
            processingExecutor.shutdown();
            if (!processingExecutor.awaitTermination(60, TimeUnit.SECONDS)) {
                processingExecutor.shutdownNow();
            }
        } catch (InterruptedException e) {
            processingExecutor.shutdownNow();
            Thread.currentThread().interrupt();
        }
    }
}








import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.model.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicLong;

@Service
public class S3Processor {

    private static final Logger log = LoggerFactory.getLogger(S3Processor.class);
    private final AmazonS3 s3Client;
    private final int chunkSize = 300; // From YAML
    private final long maxProcessedFiles = 1_000_000; // From YAML
    private final AtomicLong totalProcessed = new AtomicLong(0);
    private final ExecutorService executor = Executors.newFixedThreadPool(8); // Adjust based on load

    @Autowired
    public S3Processor(AmazonS3 s3Client) {
        this.s3Client = s3Client;
    }

    @Async
    public CompletableFuture<Void> execute(String bucketName, String prefix) {
        return fetchAndProcess(bucketName, prefix, null)
            .thenAccept(total -> log.info("Processed {} files from {}/{}", total, bucketName, prefix));
    }

    private CompletableFuture<Long> fetchAndProcess(String bucketName, String prefix, String continuationToken) {
        if (totalProcessed.get() >= maxProcessedFiles) {
            return CompletableFuture.completedFuture(totalProcessed.get());
        }

        ListObjectsV2Request request = new ListObjectsV2Request()
            .withBucketName(bucketName)
            .withPrefix(prefix)
            .withMaxKeys(1000)
            .withContinuationToken(continuationToken);

        // Async fetch
        return CompletableFuture.supplyAsync(() -> s3Client.listObjectsV2(request), executor)
            .thenCompose(result -> {
                List<S3ObjectSummary> keys = result.getObjectSummaries();
                if (keys.isEmpty()) {
                    return CompletableFuture.completedFuture(totalProcessed.get());
                }

                // Process keys in chunks asynchronously
                List<CompletableFuture<Long>> chunkFutures = new ArrayList<>();
                List<S3ObjectSummary> chunk = new ArrayList<>();
                for (S3ObjectSummary summary : keys) {
                    chunk.add(summary);
                    if (chunk.size() >= chunkSize) {
                        chunkFutures.add(processChunkAsync(new ArrayList<>(chunk)));
                        chunk.clear();
                    }
                }
                if (!chunk.isEmpty()) {
                    chunkFutures.add(processChunkAsync(chunk));
                }

                // Wait for all chunks to complete, then recurse
                return CompletableFuture.allOf(chunkFutures.toArray(new CompletableFuture[0]))
                    .thenCompose(v -> {
                        long currentProcessed = chunkFutures.stream()
                            .map(CompletableFuture::join)
                            .mapToLong(Long::longValue)
                            .sum();
                        totalProcessed.addAndGet(currentProcessed);

                        if (result.getNextContinuationToken() != null && totalProcessed.get() < maxProcessedFiles) {
                            return fetchAndProcess(bucketName, prefix, result.getNextContinuationToken());
                        }
                        return CompletableFuture.completedFuture(totalProcessed.get());
                    });
            });
    }

    @Async
    private CompletableFuture<Long> processChunkAsync(List<S3ObjectSummary> chunk) {
        return CompletableFuture.supplyAsync(() -> {
            // Your existing chunk processing logic
            log.debug("Processed chunk of size {}", chunk.size());
            return (long) chunk.size();
        }, executor);
    }

    @PreDestroy
    public void cleanup() {
        executor.shutdown();
    }
}




import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.model.*;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;

import java.util.List;
import java.util.concurrent.*;

@Service
public class S3FileProcessor {

    private final AmazonS3 s3Client;
    private final int fetcherThreadCount = 3;   // Configurable thread count for fetching
    private final int processorThreadCount = 5; // Configurable thread count for processing
    private final int chunkSize = 300;          // Configurable chunk size
    private final int maxQueueSize = 3;         // Limit to prevent memory overflow

    public S3FileProcessor(AmazonS3 s3Client) {
        this.s3Client = s3Client;
    }

    @Scheduled(cron = "0 0 * * * ?") // Adjust cron as needed
    public void processS3Files() {
        ExecutorService fetcherPool = Executors.newFixedThreadPool(fetcherThreadCount);
        ExecutorService processorPool = Executors.newFixedThreadPool(processorThreadCount);
        BlockingQueue<List<S3ObjectSummary>> queue = new LinkedBlockingQueue<>(maxQueueSize);
        int maxFilesToProcess = 1_000_000; // Configurable max records

        fetcherPool.submit(() -> fetchFromS3("bucket-name", "prefix/", queue, maxFilesToProcess));
        processQueue(queue, processorPool, maxFilesToProcess);

        fetcherPool.shutdown();
        processorPool.shutdown();
    }

    private void fetchFromS3(String bucketName, String prefix, BlockingQueue<List<S3ObjectSummary>> queue, int maxFilesToProcess) {
        String continuationToken = null;
        int totalFetched = 0;

        do {
            ListObjectsV2Request request = new ListObjectsV2Request()
                    .withBucketName(bucketName)
                    .withPrefix(prefix)
                    .withContinuationToken(continuationToken)
                    .withMaxKeys(1000);

            ListObjectsV2Result result = s3Client.listObjectsV2(request);
            List<S3ObjectSummary> objectSummaries = result.getObjectSummaries();
            totalFetched += objectSummaries.size();

            try {
                queue.put(objectSummaries); // Blocks if full
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }

            continuationToken = result.getNextContinuationToken();
        } while (continuationToken != null && totalFetched < maxFilesToProcess);
    }

    private void processQueue(BlockingQueue<List<S3ObjectSummary>> queue, ExecutorService processorPool, int maxFilesToProcess) {
        int totalProcessed = 0;

        while (totalProcessed < maxFilesToProcess) {
            try {
                List<S3ObjectSummary> batch = queue.poll(10, TimeUnit.SECONDS); // Wait for new data
                if (batch == null) break;

                List<List<S3ObjectSummary>> chunks = splitIntoChunks(batch, chunkSize);
                for (List<S3ObjectSummary> chunk : chunks) {
                    processorPool.submit(() -> processChunk(chunk));
                }

                totalProcessed += batch.size();
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
        }
    }

    private List<List<S3ObjectSummary>> splitIntoChunks(List<S3ObjectSummary> list, int chunkSize) {
        // Split list into chunks of 'chunkSize'
    }

    private void processChunk(List<S3ObjectSummary> chunk) {
        // Your processing logic
    }
}












import org.springframework.beans.factory.annotation.Value;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Component;
import software.amazon.awssdk.core.async.SdkPublisher;
import software.amazon.awssdk.services.s3.S3AsyncClient;
import software.amazon.awssdk.services.s3.model.ListObjectsV2Request;
import software.amazon.awssdk.services.s3.model.ListObjectsV2Response;
import software.amazon.awssdk.services.s3.paginators.ListObjectsV2Publisher;

import java.time.Duration;
import java.time.Instant;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicLong;
import java.util.stream.Collectors;

@Component
public class S3BatchProcessor {

    private final S3AsyncClient s3AsyncClient;
    private final ExecutorService processingPool;
    private final AtomicLong totalProcessed = new AtomicLong(0);

    @Value("${aws.s3.bucket}")
    private String bucketName;

    @Value("${processing.chunkSize:300}")
    private int chunkSize;

    @Value("${processing.maxRecords:3000000}")
    private long maxRecords;

    @Value("${processing.timeLimitMinutes:60}")
    private long timeLimitMinutes;

    public S3BatchProcessor() {
        this.s3AsyncClient = S3AsyncClient.builder()
                .httpClientBuilder(b -> b.maxConcurrency(100)
                .build();
        
        this.processingPool = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors() * 2);
    }

    @Scheduled(cron = "${cron.job1}")
    public void processJob1() {
        processFolder("folder1-prefix/");
    }

    @Scheduled(cron = "${cron.job2}")
    public void processJob2() {
        processFolder("folder2-prefix/");
    }

    @Scheduled(cron = "${cron.job3}")
    public void processJob3() {
        processFolder("folder3-prefix/");
    }

    private void processFolder(String prefix) {
        final Instant startTime = Instant.now();
        final Instant timeoutTime = startTime.plus(Duration.ofMinutes(timeLimitMinutes));
        
        ListObjectsV2Request request = ListObjectsV2Request.builder()
                .bucket(bucketName)
                .prefix(prefix)
                .build();

        ListObjectsV2Publisher publisher = s3AsyncClient.listObjectsV2Paginator(request);
        List<CompletableFuture<Void>> futures = new ArrayList<>();

        publisher.subscribe(response -> {
            if (timeoutReached(startTime) || processingComplete()) return;

            List<CompletableFuture<Void>> chunkFutures = processResponse(response, startTime);
            futures.addAll(chunkFutures);
        });

        // Wait for all processing to complete
        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();
    }

    private List<CompletableFuture<Void>> processResponse(ListObjectsV2Response response, Instant startTime) {
        List<String> keys = response.contents().stream()
                .map(s3Object -> s3Object.key())
                .collect(Collectors.toList());

        return partitionAndProcess(keys, startTime);
    }

    private List<CompletableFuture<Void>> partitionAndProcess(List<String> keys, Instant startTime) {
        List<CompletableFuture<Void>> futures = new ArrayList<>();
        List<List<String>> chunks = partition(keys, chunkSize);

        for (List<String> chunk : chunks) {
            if (processingComplete() || timeoutReached(startTime)) break;

            futures.add(CompletableFuture.runAsync(() -> processChunk(chunk), processingPool));
        }
        return futures;
    }

    private void processChunk(List<String> chunk) {
        try {
            // Your actual processing logic here
            // Example: database operations, file processing, etc.
            
            // Simulate processing
            Thread.sleep(10); 
            
            long processed = totalProcessed.addAndGet(chunk.size());
            if (processed % 10000 == 0) {
                System.out.println("Processed: " + processed + " records");
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }

    private boolean processingComplete() {
        return totalProcessed.get() >= maxRecords;
    }

    private boolean timeoutReached(Instant startTime) {
        return Duration.between(startTime, Instant.now()).toMinutes() >= timeLimitMinutes;
    }

    private static <T> List<List<T>> partition(List<T> list, int size) {
        List<List<T>> partitions = new ArrayList<>();
        for (int i = 0; i < list.size(); i += size) {
            partitions.add(list.subList(i, Math.min(i + size, list.size())));
        }
        return partitions;
    }

    // Graceful shutdown
    @PreDestroy
    public void cleanup() {
        processingPool.shutdown();
        s3AsyncClient.close();
    }
}












@Service
public class S3Processor {

    @Autowired
    private AmazonS3 s3Client;
    @Value("${chunk.size:300}")
    private int chunkSize;
    @Value("${max.processed.files:1000000}")
    private long maxProcessedFiles;

    @Async
    public void execute(String bucketName, String prefix) {
        List<S3ObjectSummary> allKeys = new ArrayList<>();
        String continuationToken = null;
        long totalProcessed = 0;

        do {
            ListObjectsV2Request request = new ListObjectsV2Request()
                .withBucketName(bucketName)
                .withPrefix(prefix)
                .withMaxKeys(1000)
                .withContinuationToken(continuationToken);

            ListObjectsV2Result result = s3Client.listObjectsV2(request);
            allKeys.addAll(result.getObjectSummaries());
            continuationToken = result.getNextContinuationToken();

            // Process in chunks
            List<List<S3ObjectSummary>> chunks = Lists.partition(allKeys, chunkSize);
            totalProcessed += processChunksInParallel(chunks);

            allKeys.clear(); // Clear memory after processing
            if (totalProcessed >= maxProcessedFiles) {
                break;
            }
        } while (continuationToken != null);

        log.info("Processed {} files from {}/{}", totalProcessed, bucketName, prefix);
    }

    private long processChunksInParallel(List<List<S3ObjectSummary>> chunks) {
        List<CompletableFuture<Long>> futures = chunks.stream()
            .map(chunk -> CompletableFuture.supplyAsync(() -> processChunk(chunk), Executors.newFixedThreadPool(4)))
            .collect(Collectors.toList());

        return futures.stream()
            .map(CompletableFuture::join)
            .mapToLong(Long::longValue)
            .sum();
    }

    private long processChunk(List<S3ObjectSummary> chunk) {
        // Your existing chunk processing logic
        return chunk.size();
    }
}